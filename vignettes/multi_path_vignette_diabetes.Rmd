---
title: "Multi-path Model Selection for Diabetes Progression"
author: "Lijuan Wang, Evan Jerome, Kira Noordwijk"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Multi-path Model Selection for Diabetes Progression}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include = FALSE}
library(finalprojectgroup12)
library(care)    # for the efron2004 diabetes data
library(knitr)

set.seed(123)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = TRUE
)
```

# 1. Import Data: Diabetes Progression
```{r}
## 01_data.R
## Load data and basic objects

library(finalprojectgroup12)
library(care)

set.seed(123)

# Load the diabetes data
data(efron2004)

# Basic structure
str(efron2004)
dim(efron2004$x)
colnames(efron2004$x)
length(efron2004$y)

# Base design matrix and response
X_base <- efron2004$x
y      <- as.numeric(efron2004$y)

p_base <- ncol(X_base)
n      <- nrow(X_base)

p_base
n

```

# 2. Feature Engineering: Second-order Terms
```{r}
## 02_features.R
## Requires: X_base, p_base, n from 01_data.R
## Builds: X (full second-order design matrix)

# If running separately, uncomment:
# source("01_data.R")

# Linear terms
X_lin <- X_base

# Quadratic terms
X_quad <- X_base^2
colnames(X_quad) <- paste0(colnames(X_base), "^2")

# Pairwise interactions
num_int   <- p_base * (p_base - 1) / 2
X_int     <- matrix(NA_real_, nrow = n, ncol = num_int)
int_names <- character(num_int)

k <- 1
for (i in 1:(p_base - 1)) {
  for (j in (i + 1):p_base) {
    X_int[, k]   <- X_base[, i] * X_base[, j]
    int_names[k] <- paste0(colnames(X_base)[i], ":", colnames(X_base)[j])
    k <- k + 1
  }
}
colnames(X_int) <- int_names

# Full second-order design matrix
X <- cbind(X_lin, X_quad, X_int)

# Quick checks
dim(X)
head(colnames(X))

```

# 3. Train/Test Split
```{r}
## 03_split.R
## Requires: X, y, n from previous scripts
## Builds: X_train, y_train, X_test, y_test

# If running separately, uncomment:
# source("01_data.R")
# source("02_features.R")

set.seed(123)

train_frac <- 0.7
n_train    <- floor(train_frac * n)

train_idx <- sample(seq_len(n), size = n_train)

X_train <- X[train_idx, ]
y_train <- y[train_idx]

X_test  <- X[-train_idx, ]
y_test  <- y[-train_idx]

c(n_train = nrow(X_train), n_test = nrow(X_test))

```

# 4. Multi-path Forward Selection (Gaussian)
```{r}
## 04_multipath.R
## Requires: X_train, y_train from previous scripts
## Builds: forest (multi-path object)

# If running separately, uncomment:
# library(finalprojectgroup12)
# source("01_data.R")
# source("02_features.R")
# source("03_split.R")

forest <- build_paths(
  x      = X_train,
  y      = y_train,
  family = "gaussian",
  K      = 10,
  eps    = 1e-6,
  delta  = 1,
  L      = 50
)

# Inspect top 5 models by AIC
head(forest$aic_by_model, 5)

```

# 5. Stability Selection
```{r}
## 05_stability.R
## Requires: X_train, y_train
## Builds: stab, stab_sorted

# If running separately, uncomment:
# library(finalprojectgroup12)
# source("01_data.R")
# source("02_features.R")
# source("03_split.R")

stab <- stability(
  x        = X_train,
  y        = y_train,
  B        = 50,
  resample = "bootstrap",
  build_args = list(
    family = "gaussian",
    K      = 10,
    eps    = 1e-6,
    delta  = 1,
    L      = 50
  )
)


stab_sorted <- sort(stab$pi, decreasing = TRUE)

# Inspect the most stable variables
head(stab_sorted)

```

6. Plausible Model Set
```{r}
## 06_plausible_models.R - Robust version

# Initial parameters
Delta_val <- 2
tau_val   <- 0.6

# Attempt to generate plausible models
plaus <- plausible_models(forest, pi = stab$pi, Delta = Delta_val, tau = tau_val)

# Check if the result is empty
if(nrow(plaus) == 0) {
  message("Warning: plausible_models() returned an empty data frame.")
  
  # Optional: try relaxing the parameters
  Delta_val <- 10   # increase AIC threshold
  tau_val   <- 0.5 # decrease stability threshold
  
  message(paste("Retrying with Delta =", Delta_val, "and tau =", tau_val))
  
  plaus <- plausible_models(forest, pi = stab$pi, Delta = Delta_val, tau = tau_val)
  
  if(nrow(plaus) == 0) {
    message("Still empty. Consider further relaxing Delta or tau, or check forest/stab inputs.")
  } else {
    message("Plausible models found with relaxed parameters.")
  }
} else {
  message("Plausible models found with initial parameters.")
}

# Print the result
plaus

```

# 7. Test-set Performance Template
```{r}
## 07_test_performance.R
## Requires: forest, X_train, y_train, X_test, y_test

# 1. Ensure X_train and X_test are data frames with correct column names
X_train_df <- as.data.frame(X_train)
X_test_df  <- as.data.frame(X_test)

# 2. Get the best model from forest
best_model <- forest$models[[1]]  # first model in forest$models

# 3. Extract variables used in the model
if (!is.null(best_model)) {
  best_model_vars <- names(coef(best_model))[-1]  # remove intercept
} else {
  best_model_vars <- character(0)
}

# 4. Construct formula for refitting
if (length(best_model_vars) == 0) {
  form <- y_train ~ 1  # intercept-only model if no variables selected
} else {
  # Keep only variables that exist in X_train_df
  best_model_vars <- intersect(best_model_vars, colnames(X_train_df))
  form <- as.formula(paste("y_train ~", paste(best_model_vars, collapse = " + ")))
}

# 5. Combine y_train with X_train_df for fitting
train_df <- cbind(y_train = y_train, X_train_df)

# 6. Refit the model
best_fit <- lm(form, data = train_df)

# 7. Predict on test set
# Ensure test set has the same columns as training
X_test_df <- X_test_df[, best_model_vars, drop = FALSE]  
y_pred_test <- predict(best_fit, newdata = X_test_df)

# 8. Compute test RMSE
rmse_test <- sqrt(mean((y_test - y_pred_test)^2))
rmse_test

```

