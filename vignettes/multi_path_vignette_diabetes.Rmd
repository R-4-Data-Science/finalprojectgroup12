---
title: "Multi-path Model Selection for Diabetes Progression"
author: "Lijuan Wang, Evan Jerome, Kira Noordwijk"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Multi-path Model Selection for Diabetes Progression}
  %\VignetteEngine{knitr::rmarkdown}
encoding: UTF-8
---

# Setup
```{r setup, include = FALSE}
library(finalprojectgroup12)
library(care)    # for the efron2004 diabetes data
library(knitr)

set.seed(123)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment  = "#>",
  echo     = TRUE
)
```

# 1. Import Data: Diabetes Progression
```{r}
## Load data and basic objects
set.seed(123)

# Load the diabetes data
data(efron2004)

# Basic structure
str(efron2004)
dim(efron2004$x)
colnames(efron2004$x)
length(efron2004$y)
class(efron2004$x)

# Base design matrix and response
X_base_raw <- efron2004$x
class(X_base_raw) <- setdiff(class(X_base_raw), "AsIs")
y          <- as.numeric(efron2004$y)

# Standardize predictors (mean 0, sd 1) to stabilize numerical behavior
X_base <- as.data.frame(scale(X_base_raw))
p_base <- ncol(X_base)
n      <- nrow(X_base)

p_base
n

```

# 2. Feature Engineering: Second-order Terms
```{r}
# Builds: X (full second-order design matrix)
# Ensure X_base is a proper data frame
X_base <- as.data.frame(X_base)

# Fresh dimensions from current X_base
p_base <- ncol(X_base)
n      <- nrow(X_base)

p_base
n

# Define linear terms
X_lin <- X_base

# Define quadratic terms
X_quad <- X_base^2
colnames(X_quad) <- paste0(colnames(X_base), "^2")

# Pairwise interactions (only if we have at least 2 predictors)
X_int <- NULL

if (p_base >= 2) {
  pairs     <- combn(seq_len(p_base), 2)
  num_int   <- ncol(pairs)
  X_int_mat <- matrix(NA_real_, nrow = n, ncol = num_int)
  int_names <- character(num_int)
  
  for (k in seq_len(num_int)) {
    i <- pairs[1, k]
    j <- pairs[2, k]
    
    X_int_mat[, k]   <- X_base[, i] * X_base[, j]
    int_names[k]     <- paste0(colnames(X_base)[i], ":", colnames(X_base)[j])
  }
  colnames(X_int_mat) <- int_names
  X_int <- as.data.frame(X_int_mat)
}

# Sanity check: all components must have n rows
stopifnot(nrow(X_lin)  == n)
stopifnot(nrow(X_quad) == n)
if (!is.null(X_int)) stopifnot(nrow(X_int) == n)

# Full second-order design matrix
if (is.null(X_int) || ncol(X_int) == 0) {
  X <- cbind(X_lin, X_quad)
} else {
  X <- cbind(X_lin, X_quad, X_int)
}

# Quick checks
dim(X)
head(colnames(X))

```

# 3. Train/Test Split
```{r}
#Setup to train on 80% of the data
set.seed(123)

train_frac <- 0.8
n_train    <- floor(train_frac * n)

# Build X_train, y_train, X_test, and y_test
train_idx <- sample(seq_len(n), size = n_train)

X_train <- X[train_idx, ]
y_train <- y[train_idx]

X_test  <- X[-train_idx, ]
y_test  <- y[-train_idx]

c(n_train = nrow(X_train), n_test = nrow(X_test))

# Use a reduced feature set for faster, more stable fits in the vignette
max_features <- min(30, ncol(X_train))

X_train_model <- X_train[, 1:max_features, drop = FALSE]
X_test_model  <- X_test[,  1:max_features, drop = FALSE]

dim(X_train_model)

```

# 4. Multi-path Forward Selection (Gaussian)
```{r}
# Multi-path model selection
forest <- suppressWarnings(
build_paths(
x      = X_train_model,  # reduced feature set
y      = y_train,
family = "gaussian",
K      = 10,
eps    = 1e-6,
delta  = 1,
L      = 30              # shorter paths for speed
)
)

# Inspect top 5 models by AIC
head(forest$aic_by_model, 5)


```

# 5. Stability Selection
```{r}
# Compute variable inclusion frequencies
stab <- suppressWarnings(
stability(
x        = X_train_model,
y        = y_train,
B        = 50,           # fewer resamples for vignette speed
resample = "bootstrap",
build_args = list(
family = "gaussian",
K      = 10,
eps    = 1e-6,
delta  = 1,
L      = 30
),
verbose = TRUE
)
)

stab_sorted <- sort(stab$pi, decreasing = TRUE)

# Inspect the most stable variables
head(stab_sorted)

```

6. Plausible Model Set
```{r}
# Sanity check: how many variables pass a looser stability threshold?
sum(stab$pi >= 0.3)
sort(stab$pi, decreasing = TRUE)[1:10]

# Construct a plausible model set with relaxed thresholds:
# - Delta = 10: allow models within 10 AIC units of the best
# - tau   = 0.3: require variables to appear with at least 30% stability
plaus <- plausible_models(
forest,
pi    = stab$pi,
Delta = 10,
tau   = 0.3
)

head(plaus)

```

# 7. Test-set Performance Template
```{r}
# 1. Make sure design matrices are data frames
X_train_df <- as.data.frame(X_train_model)
X_test_df  <- as.data.frame(X_test_model)

# 2. Choose variables based on stability, not AIC
#    First, try a threshold; if nothing passes, fall back to top 5
tau_perf <- 0.3

stable_vars <- names(stab$pi[stab$pi >= tau_perf])

if (length(stable_vars) == 0) {
  # Fall back to top 5 most stable variables
  stab_sorted <- sort(stab$pi, decreasing = TRUE)
  stable_vars <- names(head(stab_sorted, 5))
}

# Keep only variables actually present in the reduced design matrix
stable_vars <- intersect(stable_vars, colnames(X_train_df))

stable_vars

# ---- NEW PART: make variable names formula-safe ----
# Any name that isn't a simple identifier gets wrapped in backticks
is_simple_name <- grepl("^[[:alpha:]][[:alnum:]_]*$", stable_vars)

stable_vars_formula <- ifelse(
  is_simple_name,
  stable_vars,
  paste0("`", stable_vars, "`")
)

# 3. Build formula
if (length(stable_vars) == 0) {
  form <- y_train ~ 1   # intercept-only model if no variables selected
} else {
  form <- as.formula(
    paste("y_train ~", paste(stable_vars_formula, collapse = " + "))
  )
}

form

# 4. Combine y and X for training
train_df <- cbind(y_train = y_train, X_train_df)

# 5. Fit the model
best_fit <- lm(form, data = train_df)

# 6. Prepare test data with same predictors
#    NOTE: subsetting uses the *original* names (without backticks)
if (length(stable_vars) == 0) {
  test_df <- X_test_df[, 0, drop = FALSE]  # no predictors
} else {
  test_df <- X_test_df[, stable_vars, drop = FALSE]
}

# 7. Predict on test set
y_pred_test <- predict(best_fit, newdata = test_df)

# 8. Compute test RMSE
rmse_test <- sqrt(mean((y_test - y_pred_test)^2))

list(
  selected_vars = stable_vars,
  rmse_test     = rmse_test
)

```

References:
https://chatgpt.com/share/6931a395-ca80-800a-aa05-56b1fa7e7bbf
https://chatgpt.com/share/6931a3b2-7ef4-800a-ad67-2fb267610a1c
https://chatgpt.com/share/6931a3c9-a628-800a-a966-99947e463e31
https://chatgpt.com/share/6931a3e4-de9c-800a-8272-9cf31ff0a1bd
https://chatgpt.com/share/6931a782-3128-800a-804e-5fcd8adc24a7
