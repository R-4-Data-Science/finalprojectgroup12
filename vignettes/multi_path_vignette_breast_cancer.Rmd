---
title: "Breast Cancer with multi-path Model Selection"
author: "Lijuan Wang, Evan Jerome, Kira Noordwijk"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Breast Cancer with multi-path model selection}
  %\VignetteEngine{knitr::rmarkdown}
encoding: UTF-8
---

# Setup
```{r setup, include = FALSE}
library(finalprojectgroup12)
library(mlbench) # for BreastCancer data
library(knitr)
set.seed(123)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
echo = TRUE
)
```

# 1. Import Data: Breast Cancer Progression
```{r}
# Load classic Wisconsin Breast Cancer data (mlbench::BreastCancer)
data("BreastCancer")
bc <- na.omit(BreastCancer)

# Assign outcomes: malignant = 1, benign = 0
y <- ifelse(bc$Class == "malignant", 1, 0)

# Drop ID and Class columns, convert predictors to numeric
X_base_raw <- bc[, setdiff(names(bc), c("Id", "Class"))]

X_base_raw <- as.data.frame(
lapply(X_base_raw, function(col) as.numeric(as.character(col)))
)

# Standardize predictors (mean 0, sd 1) to stabilize numerical behavior
X_base <- as.data.frame(scale(X_base_raw))
rownames(X_base) <- NULL

n      <- nrow(X_base)
p_base <- ncol(X_base)

n
p_base

```

# 2. Feature Engineering: Second-order Terms
```{r}
# Define linear terms
X_lin <- X_base

# Define quadratic terms
X_quad <- X_base^2
colnames(X_quad) <- paste0(colnames(X_base), "^2")

# Pairwise interactions
num_int   <- p_base * (p_base - 1) / 2
X_int     <- matrix(NA_real_, nrow = n, ncol = num_int)
int_names <- character(num_int)

k <- 1
for (i in 1:(p_base - 1)) {
for (j in (i + 1):p_base) {
X_int[, k]   <- X_base[, i] * X_base[, j]
int_names[k] <- paste0(colnames(X_base)[i], ":", colnames(X_base)[j])
k <- k + 1
}
}
colnames(X_int) <- int_names

# Full matrix
X <- cbind(X_lin, X_quad, X_int)

dim(X)

```

# 3. Train/Test Split
```{r}
# Setup to train using 80% of data
set.seed(123)

train_frac <- 0.8
n_train    <- floor(train_frac * n)

# Build X_train, y_train, X_test, and y_test
train_idx <- sample(seq_len(n), size = n_train)

X_train <- X[train_idx, ]
y_train <- y[train_idx]

X_test  <- X[-train_idx, ]
y_test  <- y[-train_idx]  

c(n_train = nrow(X_train), n_test = nrow(X_test))

# Use a reduced feature set for faster, more stable fits in the vignette
max_features <- min(30, ncol(X_train))

X_train_model <- X_train[, 1:max_features, drop = FALSE]
X_test_model  <- X_test[,  1:max_features, drop = FALSE]

dim(X_train_model)

```

# 4. Multi-path Forward Selection (Binomial)
```{r}
# Multi-path model selection
forest_bin <- suppressWarnings(
  build_paths(
    x         = X_train,   # <- was X_train_model
    y         = y_train,
    family    = "binomial",
    K         = 5,
    eps       = 1e-6,
    delta     = 1,
    L         = 30,
    keep_fits = TRUE
  )
)

# Show top 5 by AIC
head(forest_bin$aic_by_model, 5)

```

# 5. Stability Selection
```{r}
# Compute variable inclusion frequencies
stab_bin <- suppressWarnings(
  stability(
    x        = X_train,   # <- was X_train_model
    y        = y_train,
    B        = 50,
    resample = "bootstrap",
    build_args = list(
      family    = "binomial",
      K         = 5,
      eps       = 1e-6,
      delta     = 1,
      L         = 30,
      keep_fits = FALSE
    )
  )
)

# Inspect top stabilities
stab_bin_sorted <- sort(stab_bin$pi, decreasing = TRUE)
head(stab_bin_sorted)

```

6. Plausible Model Set
```{r}
# Sanity check: how many variables pass a looser stability threshold?
sum(stab_bin$pi >= 0.3)
sort(stab_bin$pi, decreasing = TRUE)[1:10]

# Construct a plausible model set with relaxed thresholds:
# - Delta = 10: allow models within 10 AIC units of the best
# - tau   = 0.3: require variables to appear with at least 30% stability
plaus_bin <- plausible_models(
  forest_bin,
  pi    = stab_bin$pi,
  Delta = 10,
  tau   = 0.3
)

plaus_bin

```

# 7. Test-set Performance Template
```{r}
# 0. Check class balance
table(y_train)
table(y_test)

# 1. Make sure responses are 0/1 numeric
y_train_bin <- ifelse(y_train == 1, 1, 0)
y_test_bin  <- ifelse(y_test  == 1, 1, 0)

# 2. Use the SAME design used in build_paths() and stability(): X_train, X_test
X_train_df <- as.data.frame(X_train)
X_test_df  <- as.data.frame(X_test)

# 3. Choose variables based on stability
tau_perf <- 0.3

stable_vars <- names(stab_bin$pi[stab_bin$pi >= tau_perf])

if (length(stable_vars) == 0) {
  # If nothing passes the threshold, fall back to top 5 most stable variables
  stab_bin_sorted <- sort(stab_bin$pi, decreasing = TRUE)
  stable_vars <- names(head(stab_bin_sorted, 5))
}

# Keep only variables that actually exist in X_train_df
stable_vars <- intersect(stable_vars, colnames(X_train_df))

# If still nothing, just use the top few columns as a last resort
if (length(stable_vars) == 0) {
  stable_vars <- colnames(X_train_df)[1:min(5, ncol(X_train_df))]
}

stable_vars

# 4. Build training data and fit logistic regression
train_df <- data.frame(
  y = y_train_bin,
  X_train_df[, stable_vars, drop = FALSE]
)

# y ~ . uses all remaining columns as predictors; R handles weird names itself
best_fit <- glm(
  y ~ .,
  data    = train_df,
  family  = binomial,
  control = glm.control(maxit = 50)  # a few extra iterations, just in case
)

# 5. Matching test data
test_df <- data.frame(
  X_test_df[, stable_vars, drop = FALSE]
)

# 6. Predict on test set
prob_test <- predict(best_fit, newdata = test_df, type = "response")
pred_test <- ifelse(prob_test >= 0.5, 1, 0)

# 7. Confusion matrix and metrics
TP <- sum(pred_test == 1 & y_test_bin == 1)
TN <- sum(pred_test == 0 & y_test_bin == 0)
FP <- sum(pred_test == 1 & y_test_bin == 0)
FN <- sum(pred_test == 0 & y_test_bin == 1)

conf_mat <- matrix(
  c(TP, FP, FN, TN),
  nrow = 2,
  byrow = TRUE,
  dimnames = list(c("Pred 1", "Pred 0"), c("True 1", "True 0"))
)

conf_mat

accuracy    <- (TP + TN) / (TP + TN + FP + FN)
sensitivity <- if ((TP + FN) == 0) NA else TP / (TP + FN)
specificity <- if ((TN + FP) == 0) NA else TN / (TN + FP)
FDR         <- if ((FP + TP) == 0) NA else FP / (FP + TP)
DOR         <- if (FP * FN == 0)   NA else (TP / FN) / (FP / TN)

list(
  selected_vars = stable_vars,
  accuracy      = accuracy,
  sensitivity   = sensitivity,
  specificity   = specificity,
  FDR           = FDR,
  DOR           = DOR
)

```

References:
https://chatgpt.com/share/6931a395-ca80-800a-aa05-56b1fa7e7bbf
https://chatgpt.com/share/6931a3b2-7ef4-800a-ad67-2fb267610a1c
https://chatgpt.com/share/6931a3c9-a628-800a-a966-99947e463e31
https://chatgpt.com/share/6931a3e4-de9c-800a-8272-9cf31ff0a1bd
https://chatgpt.com/share/6931a782-3128-800a-804e-5fcd8adc24a7
